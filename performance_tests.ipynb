{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02edd346",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a3734",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This cell must be run prior to the others. Modify `visualise` depending on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent import Agent\n",
    "from src.env import MultipleOvercookedEnv, OvercookedEnvFactory\n",
    "from src.parameters import Hyperparameters, Options\n",
    "from src.testing import AgentTester\n",
    "from src.training import AgentTrainer, ParallelAgentTrainer\n",
    "from src.utils import create_writer, plot_rewards, run_tensorboard\n",
    "\n",
    "# This cell is needed for all other cells\n",
    "factory = OvercookedEnvFactory(info_level=0, horizon=400, old_dynamics=True)\n",
    "visualise = False\n",
    "# If set to true, games will be visualised via Pygame\n",
    "# If set to false, a short description will instead be printed about played games.\n",
    "# Be wary that due to some issues in Jupyter Notebook, \n",
    "# pygame windows will stay open forever unless you close them manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ea267",
   "metadata": {},
   "source": [
    "## Cramped Room Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619890fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_coordination = factory.create_env(\"cramped_room\")\n",
    "\n",
    "options = Options(\n",
    "    save_agent_after_training=True, \n",
    "    tqdm_description=\"Cramped Room\",\n",
    "    checkpoints_dirname=\"cramped_room_first\",\n",
    "    total_episodes=1000\n",
    ")\n",
    "parameters = Hyperparameters()\n",
    "cramped_room_agent = Agent(\n",
    "    parameters=parameters,\n",
    "    options=options,\n",
    "    n_actions=forced_coordination.action_space.n,\n",
    "    input_dim=forced_coordination.observation_space.shape,\n",
    "    writer_factory=lambda: create_writer(name=\"Cramped Room Agent\")\n",
    ")\n",
    "cramped_room_agent.load_if_save_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cramped_room_agent.loaded_from_save:\n",
    "    cramped_room_trainer = AgentTrainer(forced_coordination, cramped_room_agent)\n",
    "    cramped_room_trainer.train_agent()\n",
    "    run_tensorboard()\n",
    "else:\n",
    "    print(\"Agent has already been trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e349bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = AgentTester(forced_coordination, cramped_room_agent)\n",
    "game = tester.play_game()\n",
    "game.visualise_or_report(visualise=visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30590509",
   "metadata": {},
   "source": [
    "## Overfitting in Cramped Room\n",
    "This is demonstrated by not using minibatches and penalising the agent less for being deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for is_overfitted in (True, False):\n",
    "    forced_coordination = factory.create_env(\"cramped_room\")\n",
    "    if is_overfitted:\n",
    "        parameters = Hyperparameters(entropy_coefficient=0.001, batch_size=4000)\n",
    "    else:\n",
    "        parameters = Hyperparameters()\n",
    "\n",
    "    overfit_options = Options(\n",
    "        rollout_episodes=10, \n",
    "        total_episodes=1000,\n",
    "        use_batches=not is_overfitted,\n",
    "        save_agent_after_training=True,\n",
    "        checkpoints_dirname=f\"{'non' if not is_overfitted else ''}overfit_cramped_room\",\n",
    "        tqdm_description=\"Overfit Agent\" if is_overfitted else \"Non-overfit agent\",\n",
    "    )\n",
    "    overfit_agent = Agent(\n",
    "        parameters=parameters, \n",
    "        options=overfit_options, \n",
    "        env=forced_coordination,\n",
    "        writer_factory=lambda: create_writer(\n",
    "            name=\"Overfitted Cramped Room\" if is_overfitted else \"Non-overfitted Cramped Room\"\n",
    "        )\n",
    "    )\n",
    "    overfit_agent.load_if_save_exists()\n",
    "\n",
    "    if not overfit_agent.loaded_from_save:\n",
    "        overfit_trainer = AgentTrainer(forced_coordination, overfit_agent)\n",
    "        overfit_trainer.train_agent()\n",
    "    else:\n",
    "        print(\"Agent was already trained.\")\n",
    "\n",
    "    tester = AgentTester(\n",
    "    forced_coordination, \n",
    "        overfit_agent, \n",
    "        always_choose_best_actions=is_overfitted\n",
    "    )\n",
    "    game = tester.play_game()\n",
    "    game.visualise_or_report(visualise=False)\n",
    "run_tensorboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = AgentTester(\n",
    "    forced_coordination, \n",
    "    overfit_agent, \n",
    "    always_choose_best_actions=True\n",
    ")\n",
    "game = tester.play_game()\n",
    "game.visualise_or_report(visualise=visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76715080",
   "metadata": {},
   "source": [
    "## Multiple Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca93935",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_env = MultipleOvercookedEnv(\"forced_coordination\", \"forced_coordination\", \"asymmetric_advantages\")\n",
    "\n",
    "options = Options(\n",
    "    save_agent_after_training=True, \n",
    "    checkpoints_dirname=\"multi_envs\",\n",
    "    total_episodes=1000\n",
    ")\n",
    "\n",
    "multi_env_agent = Agent(\n",
    "    parameters=Hyperparameters(), \n",
    "    options=options, \n",
    "    env=multi_env,\n",
    "    writer_factory=lambda: create_writer(name=\"Multiple Envs\")\n",
    ")\n",
    "multi_env_agent.load_if_save_exists()\n",
    "\n",
    "if not multi_env_agent.loaded_from_save: \n",
    "    trainer = AgentTrainer(multi_env, multi_env_agent)\n",
    "    trainer.train_agent()\n",
    "else:\n",
    "    print(\"Agent was already saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ee31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in multi_env:\n",
    "    multi_env_tester = AgentTester(env, multi_env_agent)\n",
    "    game = multi_env_tester.play_game()\n",
    "    game.visualise_or_report(visualise=visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35616044",
   "metadata": {},
   "source": [
    "## Removing vs keeping shaped rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537597dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Hyperparameters()\n",
    "# Because we are using subprocesses\n",
    "options = Options(is_running_jupyter_notebook=False)\n",
    "\n",
    "trainer = ParallelAgentTrainer(\n",
    "    \"forced_coordination\",\n",
    "    \"forced_coordination\", \n",
    "    parameters=Hyperparameters(),\n",
    "    options=[\n",
    "        options.with_values(tqdm_description=\"Always shaped\"), \n",
    "        options.with_values(tqdm_description=\"Stop shaped\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.train_agents()\n",
    "trainer.wait_until_finished_training()\n",
    "run_tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55256f12",
   "metadata": {},
   "source": [
    "## All layout results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dtypes import LayoutName, Reward\n",
    "from src.utils import get_proper_layout_name\n",
    "\n",
    "layout_names: list[LayoutName] = [\n",
    "    \"asymmetric_advantages\", \n",
    "    \"coordination_ring\", \n",
    "    \"counter_circuit_o_1order\", \n",
    "    \"cramped_room\", \n",
    "    \"forced_coordination\"\n",
    "]\n",
    "\n",
    "# Results will vary depending on this value (also as shown in the report)\n",
    "critic_fc_dim = 192\n",
    "options = Options( \n",
    "    save_agent_after_training=False, \n",
    "    total_episodes=1000\n",
    ")\n",
    "parameters = Hyperparameters(\n",
    "    critic_fc1_dim=critic_fc_dim,\n",
    "    critic_fc2_dim=critic_fc_dim\n",
    ")\n",
    "\n",
    "rewards_and_layouts: list[tuple[LayoutName, list[Reward]]] = []\n",
    "\n",
    "for layout_name in layout_names:\n",
    "    layout_proper_name = get_proper_layout_name(layout_name)\n",
    "    options = options.with_values(\n",
    "        tqdm_description=layout_proper_name\n",
    "    )\n",
    "\n",
    "    env = factory.create_env(layout_name)\n",
    "    agent = Agent(\n",
    "        parameters=parameters,\n",
    "        options=options, \n",
    "        env=env,\n",
    "        writer_factory=lambda: create_writer(name=layout_proper_name)\n",
    "    )\n",
    "\n",
    "    trainer = AgentTrainer(env, agent)\n",
    "    rewards = trainer.train_agent()\n",
    "    rewards_and_layouts.append((layout_name, rewards))\n",
    "\n",
    "plot_rewards(rewards_and_layouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77583eac",
   "metadata": {},
   "source": [
    "## Generalisation capabilities with 2 different environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordination_ring, forced_coordination = factory.create_envs(\"coordination_ring\", \"forced_coordination\")\n",
    "options = Options(\n",
    "    save_agent_after_training=False,\n",
    "    checkpoints_dirname=\"two_env_generalisation\",\n",
    "    total_episodes=2000\n",
    ")\n",
    "parameters = Hyperparameters()\n",
    "agent = Agent(\n",
    "    parameters=parameters,\n",
    "    options=options, \n",
    "    env=coordination_ring\n",
    ")\n",
    "agent.load_if_save_exists()\n",
    "\n",
    "multi_env = MultipleOvercookedEnv(coordination_ring, forced_coordination, reset_env_interval=1)\n",
    "if not agent.loaded_from_save:\n",
    "    trainer = AgentTrainer(multi_env, agent)\n",
    "    trainer.train_agent()\n",
    "\n",
    "for env in multi_env:\n",
    "    tester = AgentTester(env, agent)\n",
    "    game = tester.play_game()\n",
    "    game.visualise_or_report(visualise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6297a",
   "metadata": {},
   "source": [
    "## Using larger critic dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_proper_layout_name\n",
    "\n",
    "# Change the layout name to see the difference in results\n",
    "layout_name: LayoutName = \"asymmetric_advantages\"\n",
    "rewards_and_layouts = []\n",
    "for name, factor in zip(\n",
    "    [layout_name, f\"{layout_name}_(modified)\"],\n",
    "    (1, 2)\n",
    "):\n",
    "    env = factory.create_env(\"forced_coordination\")\n",
    "    parameters = Hyperparameters(\n",
    "        critic_fc1_dim=96 * factor,\n",
    "        critic_fc2_dim=96 * factor\n",
    "    )\n",
    "    options = Options(\n",
    "        tqdm_description=get_proper_layout_name(name), \n",
    "        total_episodes=1000,\n",
    "        is_running_jupyter_notebook=False\n",
    "    )\n",
    "    agent = Agent(options=options, parameters=parameters, env=env)\n",
    "    trainer = AgentTrainer(env, agent)\n",
    "    rewards = trainer.train_agent()\n",
    "    rewards_and_layouts.append((name, rewards))\n",
    "\n",
    "plot_rewards(rewards_and_layouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78f319",
   "metadata": {},
   "source": [
    "## Batching vs. No batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c393ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for is_batched in (True, False):\n",
    "    forced_coordination = factory.create_env(\"forced_coordination\")\n",
    "    parameters = Hyperparameters()\n",
    "\n",
    "    options = Options(\n",
    "        rollout_episodes=10, \n",
    "        total_episodes=1000,\n",
    "        use_batches=is_batched,\n",
    "        save_agent_after_training=True,\n",
    "        checkpoints_dirname=f\"{'non' if not is_batched else ''}batched_forced_coordination\",\n",
    "        tqdm_description=\"Batched\" if is_batched else \"Non-batched\",\n",
    "    )\n",
    "    agent = Agent(\n",
    "        parameters=parameters, \n",
    "        options=options, \n",
    "        env=forced_coordination,\n",
    "        writer_factory=lambda: create_writer(\n",
    "            name=\"Batched Forced Coordination\" if is_batched else \"Non-batched Forced Coordination\"\n",
    "        )\n",
    "    )\n",
    "    agent.load_if_save_exists()\n",
    "\n",
    "    if not agent.loaded_from_save:\n",
    "        overfit_trainer = AgentTrainer(forced_coordination, agent)\n",
    "        overfit_trainer.train_agent()\n",
    "    else:\n",
    "        print(\"Agent was already trained.\")\n",
    "\n",
    "    tester = AgentTester(forced_coordination, agent)\n",
    "    game = tester.play_game()\n",
    "    game.visualise_or_report(visualise=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aas-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
